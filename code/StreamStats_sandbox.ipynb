{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb495d-f210-41ea-9407-f31750f333c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19463bcc-d0eb-441d-aa41-a2dca417461b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getAllStreamStats(i):\n",
    "    ## Define gage location\n",
    "#     y_loc = dat[\"dec_lat_va\"][i]\n",
    "#     x_loc = dat[\"dec_long_va\"][i]\n",
    "    \n",
    "    y_loc = np.round(dat[\"dec_lat_va\"][i],2)\n",
    "    x_loc = np.round(dat[\"dec_long_va\"][i],2)\n",
    "    state = 'OR'\n",
    "    gage = dat[\"gage\"][i]\n",
    "    \n",
    "    \n",
    "    ## Define Streamstats API URLS\n",
    "    # Service URLS\n",
    "    StreamStatsServiceURLS = {\n",
    "        'watershed': 'https://streamstats.usgs.gov/streamstatsservices/watershed.geojson',\n",
    "        'basinCharacteristics': 'https://streamstats.usgs.gov/streamstatsservices/parameters.json',\n",
    "        'gage': 'https://streamstats.usgs.gov/gagestatsservices/stations'\n",
    "        }\n",
    "    NSSServiceURlS = {\n",
    "        'regressionRegions': 'https://streamstats.usgs.gov/nssservices/regressionregions/bylocation',\n",
    "        'scenarios': 'https://streamstats.usgs.gov/nssservices/scenarios',\n",
    "        'computeFlowStats': 'https://streamstats.usgs.gov/nssservices/scenarios/Estimate'\n",
    "    }\n",
    "\n",
    "    # Declare cookies for use of consistent StreamStats servers\n",
    "    global cookies \n",
    "    \n",
    "    # Step 1 - Get the watershed\n",
    "    print('Step 1 - Get the watershed')\n",
    "    watershedURLParams = {\n",
    "        'rcode': state, \n",
    "        'xlocation': x_loc, \n",
    "        'ylocation': y_loc,\n",
    "        'crs': 4326,\n",
    "    #     'crs': 4269,\n",
    "    }\n",
    "    delineatedBasin = None\n",
    "    while delineatedBasin is None: # Sometimes the watershed endpoint returns with no geometry - this loop is written to keep trying until a geometry is returned\n",
    "        print('Delineating basin...')\n",
    "        basinDelineationResponse = requests.get(url = StreamStatsServiceURLS['watershed'], params = watershedURLParams)\n",
    "        if basinDelineationResponse.status_code == 200:\n",
    "            cookies = basinDelineationResponse.cookies\n",
    "            try:\n",
    "                delineatedBasin = json.loads(basinDelineationResponse.content.decode('utf-8'))[\"featurecollection\"][1][\"feature\"][\"features\"][0][\"geometry\"] # this will be used in step 2\n",
    "                workspaceID = json.loads(basinDelineationResponse.content.decode('utf-8'))[\"workspaceID\"] # this will be used in step 4\n",
    "            except:\n",
    "                print(\"No geometry returned. Retrying...\")\n",
    "                pass\n",
    "        else:\n",
    "            print(\"Error. Retrying...\")\n",
    "            pass\n",
    "\n",
    "    # Step 2 - Get the Regression Regions associated with the watershed\n",
    "    print('Step 2 - Get the Regression Regions associated with the watershed')\n",
    "    regressionRegionPOSTBody = delineatedBasin # from step 1\n",
    "\n",
    "    regressionRegions = None\n",
    "    while regressionRegions == None:\n",
    "        regressionRegionsResponse = requests.post(url = NSSServiceURlS['regressionRegions'], json=regressionRegionPOSTBody)\n",
    "        if regressionRegionsResponse.status_code == 200:\n",
    "            regressionRegions = json.loads(regressionRegionsResponse.content.decode('utf-8'))\n",
    "            regressionRegionCodes = [ sub['code'] for sub in regressionRegions ] # this will be used in step 3\n",
    "        else:\n",
    "            print(\"Error.\")\n",
    "            pass\n",
    "\n",
    "    # Step 3 - Get the Scenarios associated with the Regression Regions\n",
    "    print('Step 3 - Get the Scenarios associated with the Regression Regions')\n",
    "\n",
    "    scenarioURLParams = {\n",
    "        'regions': state, \n",
    "        'statisticgroups': 4, # this can be updated depending on which flow statistics you want to calculate (2 is associated with Peak Flow Statistics) - you can find all statistic groups available for your region at https://streamstats.usgs.gov/docs/nssservices/#/StatisticGroups/GET/StatisticGroups\n",
    "        'regressionregions': (','.join(regressionRegionCodes)) # from step 2, needs to be comma separated list\n",
    "    }\n",
    "    scenarioResponse = requests.get(url = NSSServiceURlS['scenarios'], params=scenarioURLParams)\n",
    "\n",
    "    parameters = None\n",
    "    while parameters == None:\n",
    "        if scenarioResponse.status_code == 200:\n",
    "            scenarios = json.loads(scenarioResponse.content.decode('utf-8'))[0] # this will be used in step 5\n",
    "            parameters = json.loads(scenarioResponse.content.decode('utf-8'))[0][\"regressionRegions\"][0][\"parameters\"]\n",
    "            parameterCodes = [ sub['code'] for sub in parameters ] # this will be used in step 4\n",
    "        else:\n",
    "            print(\"Error.\")\n",
    "            pass\n",
    "\n",
    "    # Step 4 - Compute the basin characteristics\n",
    "    print('Step 4 - Compute the basin characteristics')\n",
    "    basinCharacteristicsURLParams = {\n",
    "        'rcode': state, \n",
    "        'workspaceID': workspaceID, # from step 1\n",
    "        'includeparameters': (','.join(parameterCodes)) # from step 3\n",
    "    }\n",
    "\n",
    "    basinCharacteristics = None\n",
    "    while basinCharacteristics is None:\n",
    "        basinCharacteristicsResponse = requests.get(url = StreamStatsServiceURLS['basinCharacteristics'], params=basinCharacteristicsURLParams, cookies=cookies)\n",
    "        if basinCharacteristicsResponse.status_code == 200:\n",
    "            basinCharacteristics = json.loads(basinCharacteristicsResponse.content.decode('utf-8')) # this will be used in step 5\n",
    "        else:\n",
    "            print(\"Error.\")\n",
    "            pass\n",
    "\n",
    "    # Step 5 - Compute Flow Statistics\n",
    "    print('Step 5 - Compute Flow Statistics')\n",
    "    flowStatsURLParams = {\n",
    "        'regions': state\n",
    "    }\n",
    "    flowStatsPOSTBody = [] \n",
    "    for counter, x in enumerate(scenarios['regressionRegions'][0]['parameters']): # from step 3 & step 4\n",
    "        for p in basinCharacteristics['parameters']:\n",
    "            if x['code'].lower() == p['code'].lower():\n",
    "                scenarios['regressionRegions'][0]['parameters'][counter]['value'] = p['value']\n",
    "    flowStatsPOSTBody.append(scenarios)\n",
    "\n",
    "    flowStats = None\n",
    "    while flowStats == None:\n",
    "        flowStatsResponse = requests.post(url = NSSServiceURlS['computeFlowStats'], params = flowStatsURLParams, json=flowStatsPOSTBody)\n",
    "        if flowStatsResponse.status_code == 200:\n",
    "            flowStats = json.loads(flowStatsResponse.content.decode('utf-8'))\n",
    "        else:\n",
    "            print(\"Error.\")\n",
    "            pass\n",
    "\n",
    "    out = pd.json_normalize(flowStats,\n",
    "                             record_path=['regressionRegions',['results']],\n",
    "                            )[[\"code\",\"value\"]]\n",
    "    out[\"gage\"] = gage\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "7f9826e5-7735-4a45-b52d-9bb57dd74533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-327-2de0735adb11>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[\"state\"][dat[\"state_cd\"]==16] = \"ID\"\n",
      "<ipython-input-327-2de0735adb11>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[\"state\"][dat[\"state_cd\"]==41] = \"OR\"\n",
      "<ipython-input-327-2de0735adb11>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[\"state\"][dat[\"state_cd\"]==53] = \"WA\"\n"
     ]
    }
   ],
   "source": [
    "pnwNPall = pd.read_csv(\"../data/pnwNPall_InfowStats.csv\")\n",
    "siteID = list(pnwNPall['gage'].astype(str))\n",
    "dat = nwis.get_info(sites = siteID)[0]\n",
    "\n",
    "\n",
    "dat[\"state\"] = \"none\"\n",
    "dat[\"state\"][dat[\"state_cd\"]==16] = \"ID\"\n",
    "dat[\"state\"][dat[\"state_cd\"]==41] = \"OR\"\n",
    "dat[\"state\"][dat[\"state_cd\"]==53] = \"WA\"\n",
    "\n",
    "st = [\"ID\",'WA','OR']\n",
    "dat = dat[dat[\"state\"].isin(st)].reset_index(drop=True)\n",
    "dat = dat[[\"site_no\",\"dec_lat_va\",\"dec_long_va\",'state','station_nm']]\n",
    "dat.columns = [\"gage\",\"lat\",\"long\",'state','name']\n",
    "dat['gage']  =dat['gage'].astype(int)\n",
    "dat[\"lat\"] = round(dat[\"lat\"],3)\n",
    "dat[\"long\"] = round(dat[\"long\"],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "cca4f99e-ddc7-4d8a-9b8a-97ee698dcedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gage</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>12115700</td>\n",
       "      <td>47.366</td>\n",
       "      <td>-121.693</td>\n",
       "      <td>WA</td>\n",
       "      <td>BOULDER CREEK NEAR CEDAR FALLS, WA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gage     lat     long state                                name\n",
       "61  12115700  47.366 -121.693    WA  BOULDER CREEK NEAR CEDAR FALLS, WA"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[dat[\"gage\"]==12115700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "4187d893-8087-4848-b43b-031899b01beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Get the watershed\n",
      "Delineating basin...\n",
      "Step 2 - Get the Regression Regions associated with the watershed\n",
      "Step 3 - Get the Scenarios associated with the Regression Regions\n",
      "Step 4 - Compute the basin characteristics\n",
      "Step 5 - Compute Flow Statistics\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "\n",
    "\n",
    "## Define gage location\n",
    "y_loc = dat[\"lat\"][i]\n",
    "x_loc = dat[\"long\"][i]\n",
    "state = dat[\"state\"][i]\n",
    "gage = dat[\"gage\"][i]\n",
    "\n",
    "\n",
    "## Define Streamstats API URLS\n",
    "# Service URLS\n",
    "StreamStatsServiceURLS = {\n",
    "    'watershed': 'https://streamstats.usgs.gov/streamstatsservices/watershed.geojson',\n",
    "    'basinCharacteristics': 'https://streamstats.usgs.gov/streamstatsservices/parameters.json',\n",
    "    'gage': 'https://streamstats.usgs.gov/gagestatsservices/stations'\n",
    "    }\n",
    "NSSServiceURlS = {\n",
    "    'regressionRegions': 'https://streamstats.usgs.gov/nssservices/regressionregions/bylocation',\n",
    "    'scenarios': 'https://streamstats.usgs.gov/nssservices/scenarios',\n",
    "    'computeFlowStats': 'https://streamstats.usgs.gov/nssservices/scenarios/Estimate'\n",
    "}\n",
    "\n",
    "# Declare cookies for use of consistent StreamStats servers\n",
    "global cookies \n",
    "\n",
    "# Step 1 - Get the watershed\n",
    "print('Step 1 - Get the watershed')\n",
    "watershedURLParams = {\n",
    "    'rcode': state, \n",
    "    'xlocation': x_loc, \n",
    "    'ylocation': y_loc,\n",
    "    'crs': 4326,\n",
    "#     'crs': 4269,\n",
    "}\n",
    "delineatedBasin = None\n",
    "while delineatedBasin is None: # Sometimes the watershed endpoint returns with no geometry - this loop is written to keep trying until a geometry is returned\n",
    "    print('Delineating basin...')\n",
    "    basinDelineationResponse = requests.get(url = StreamStatsServiceURLS['watershed'], params = watershedURLParams)\n",
    "    if basinDelineationResponse.status_code == 200:\n",
    "        cookies = basinDelineationResponse.cookies\n",
    "        try:\n",
    "            delineatedBasin = json.loads(basinDelineationResponse.content.decode('utf-8'))[\"featurecollection\"][1][\"feature\"][\"features\"][0][\"geometry\"] # this will be used in step 2\n",
    "            workspaceID = json.loads(basinDelineationResponse.content.decode('utf-8'))[\"workspaceID\"] # this will be used in step 4\n",
    "        except:\n",
    "            print(\"No geometry returned. Retrying...\")\n",
    "    else:\n",
    "        print(\"Error. Retrying...\")\n",
    "\n",
    "# Step 2 - Get the Regression Regions associated with the watershed\n",
    "print('Step 2 - Get the Regression Regions associated with the watershed')\n",
    "regressionRegionPOSTBody = delineatedBasin # from step 1\n",
    "\n",
    "regressionRegions = None\n",
    "while regressionRegions == None:\n",
    "    regressionRegionsResponse = requests.post(url = NSSServiceURlS['regressionRegions'], json=regressionRegionPOSTBody)\n",
    "    if regressionRegionsResponse.status_code == 200:\n",
    "        regressionRegions = json.loads(regressionRegionsResponse.content.decode('utf-8'))\n",
    "        regressionRegionCodes = [ sub['code'] for sub in regressionRegions ] # this will be used in step 3\n",
    "    else:\n",
    "        print(\"Error.\")\n",
    "        pass\n",
    "\n",
    "# Step 3 - Get the Scenarios associated with the Regression Regions\n",
    "print('Step 3 - Get the Scenarios associated with the Regression Regions')\n",
    "\n",
    "scenarioURLParams = {\n",
    "    'regions': state, \n",
    "    'statisticgroups': 4, # this can be updated depending on which flow statistics you want to calculate (2 is associated with Peak Flow Statistics) - you can find all statistic groups available for your region at https://streamstats.usgs.gov/docs/nssservices/#/StatisticGroups/GET/StatisticGroups\n",
    "    'regressionregions': (','.join(regressionRegionCodes)) # from step 2, needs to be comma separated list\n",
    "}\n",
    "scenarioResponse = requests.get(url = NSSServiceURlS['scenarios'], params=scenarioURLParams)\n",
    "\n",
    "parameters = None\n",
    "while parameters == None:\n",
    "    if scenarioResponse.status_code == 200:\n",
    "        scenarios = json.loads(scenarioResponse.content.decode('utf-8'))[0] # this will be used in step 5\n",
    "        parameters = json.loads(scenarioResponse.content.decode('utf-8'))[0][\"regressionRegions\"][0][\"parameters\"]\n",
    "        parameterCodes = [ sub['code'] for sub in parameters ] # this will be used in step 4\n",
    "    else:\n",
    "        print(\"Error.\")\n",
    "        pass\n",
    "\n",
    "# Step 4 - Compute the basin characteristics\n",
    "print('Step 4 - Compute the basin characteristics')\n",
    "basinCharacteristicsURLParams = {\n",
    "    'rcode': state, \n",
    "    'workspaceID': workspaceID, # from step 1\n",
    "    'includeparameters': (','.join(parameterCodes)) # from step 3\n",
    "}\n",
    "\n",
    "basinCharacteristics = None\n",
    "while basinCharacteristics is None:\n",
    "    basinCharacteristicsResponse = requests.get(url = StreamStatsServiceURLS['basinCharacteristics'], params=basinCharacteristicsURLParams, cookies=cookies)\n",
    "    if basinCharacteristicsResponse.status_code == 200:\n",
    "        basinCharacteristics = json.loads(basinCharacteristicsResponse.content.decode('utf-8')) # this will be used in step 5\n",
    "    else:\n",
    "        print(\"Error.\")\n",
    "        pass\n",
    "\n",
    "# Step 5 - Compute Flow Statistics\n",
    "print('Step 5 - Compute Flow Statistics')\n",
    "flowStatsURLParams = {\n",
    "    'regions': state\n",
    "}\n",
    "flowStatsPOSTBody = [] \n",
    "for counter, x in enumerate(scenarios['regressionRegions'][0]['parameters']): # from step 3 & step 4\n",
    "    for p in basinCharacteristics['parameters']:\n",
    "        if x['code'].lower() == p['code'].lower():\n",
    "            scenarios['regressionRegions'][0]['parameters'][counter]['value'] = p['value']\n",
    "flowStatsPOSTBody.append(scenarios)\n",
    "\n",
    "flowStats = None\n",
    "while flowStats == None:\n",
    "    flowStatsResponse = requests.post(url = NSSServiceURlS['computeFlowStats'], params = flowStatsURLParams, json=flowStatsPOSTBody)\n",
    "    if flowStatsResponse.status_code == 200:\n",
    "        flowStats = json.loads(flowStatsResponse.content.decode('utf-8'))\n",
    "    else:\n",
    "        print(\"Error.\")\n",
    "        pass\n",
    "\n",
    "out = pd.json_normalize(flowStats,\n",
    "                         record_path=['regressionRegions',['results']],\n",
    "                        )[[\"code\",\"value\",\"equation\"]]\n",
    "out[\"gage\"] = gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "20d5eed3-a2b5-4bbd-abb9-854e2b5ab76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = \"M7D10Y\"\n",
    "eq = out['equation'][out[\"code\"]==METRIC].values[0].replace(\"^\", \"**\")\n",
    "TT = eval(eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3308f367-8c96-4f51-ae66-d7e8a57ed2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "StreamStatsServiceURLS = {\n",
    "    'gage': 'https://streamstats.usgs.gov/gagestatsservices/characteristics'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "ceb683a3-369a-4334-8d11-b95f4f6dffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=61\n",
    "\n",
    "gageURLParams = {\n",
    "    \"stationIDOrCode\": dat[\"gage\"][i]\n",
    "}\n",
    "\n",
    "\n",
    "gageResponse = requests.get(url = StreamStatsServiceURLS['gage'], params = gageURLParams)\n",
    "\n",
    "if gageResponse.status_code == 200:\n",
    "    cookies = gageResponse.cookies\n",
    "    try:\n",
    "        gageStats = pd.DataFrame(json.loads(gageResponse.content.decode('utf-8')))\n",
    "\n",
    "\n",
    "        varCode = pd.DataFrame(list(gageStats[\"variableType\"]))[\"code\"]\n",
    "        units = pd.DataFrame(list(gageStats[\"unitType\"]))[\"name\"]\n",
    "\n",
    "        gS = pd.DataFrame(gageStats[[\"variableTypeID\",\"value\"]]).index_reset(drop=True)\n",
    "        gS[\"code\"] = varCode\n",
    "        gS[\"unit\"] = units\n",
    "#         gS =gS.set_index('code')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ce6dc835-2e77-433f-adfb-5b6422365e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipCHECK = basinCharacteristics[\"parameters\"][1][\"code\"]\n",
    "PRECIP = basinCharacteristics[\"parameters\"][1][\"value\"]\n",
    "DRNAREA = gS[\"value\"][gS['code'] == \"DRNAREA\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "386909c1-428a-4e84-b983-a8f392a6e1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I24H2Y', 'I24H50Y', 'I24H100Y', 'DRNAREA', 'LAT_CENT',\n",
       "       'LONG_CENT', 'ELEVMAX', 'ELEV', 'BSHAPE', 'JANMINTMP', 'MINBELEV',\n",
       "       'RELIEF', 'STRMTOT', 'NFSL30_30M', 'SLOP30_30M', 'BSLDEM30M',\n",
       "       'IMPNLCD01', 'BASLENAH', 'DRNDENSITY', 'PRECPRIS10', 'LC01CANOPY',\n",
       "       'SSURGINDEX'], dtype=object)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gS['code'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c0cf1ee1-42bf-4a3c-abc8-1b22b0b37c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = pd.json_normalize(flowStats,\n",
    "#                          record_path=['regressionRegions',['results']],\n",
    "#                         )[[\"code\",\"value\",\"equation\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "9e49a213-1b85-4ecc-8ff1-406d96c7f1d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-401-ad5df6bc749a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3453\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3454\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3455\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa5510-56e8-460d-801b-aeb4f18611ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
